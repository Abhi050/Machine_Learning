{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multiple_Linear_regression(ud).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhi050/Machine_Learning/blob/master/Multiple_Linear_regression(ud).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "vbMq8n4vbSdV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5_Zo3JuLbjYJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Startups.csv')\n",
        "#in this case y is the profit\n",
        "#x is the other columns of the datset of the file\n",
        "x=df.iloc[:, :-1].values\n",
        "y=df.iloc[:,4].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Gu8Rr03b2fZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "943440d9-ba37-4901-d5c4-1c1faa7f0491"
      },
      "cell_type": "code",
      "source": [
        "#categorical values in dataset x needs to be change\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "labelencoder_x=LabelEncoder()\n",
        "#replcing the values in the x matrix\n",
        "x[:,3]=labelencoder_x.fit_transform(x[:,3])\n",
        "#then we will convert those labels to dummy variable\n",
        "onehotencoder=OneHotEncoder(categorical_features=[3])\n",
        "x=onehotencoder.fit_transform(x).toarray()\n",
        "#for y we dont need it as y dont contain any categorical value\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
            "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "JLBON1Smb6Kl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Avoiding Dummy Trap, although regression lib in pyhton/sklearn already take care of the dummy variable trap\n",
        "x=x[:,1:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OswotqWPb9LF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#DISTRIBUTION\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.2,random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mrxmV1DccKcy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#FEATURE SCALING is also done by sklearn library\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RayojhdQcMA_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "21fd1f8c-dd6b-4d6c-da92-8ae549948df8"
      },
      "cell_type": "code",
      "source": [
        "#Fitting multiple linear regression to the training set\n",
        "from sklearn.linear_model import LinearRegression\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(train_x,train_y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
              "         normalize=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "fMjGr4EUcOvA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#prediction\n",
        "pred=regressor.predict(test_x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xNE85nVhcRO-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "fc961b02-ed8c-4457-d98d-2f7bf08b2071"
      },
      "cell_type": "code",
      "source": [
        "#Building the optimized version of the above model\n",
        "\"\"\"we are going to use various model building techniques like\n",
        "backward elimination, or bidirectional elimination,etc.\"\"\"\n",
        "import statsmodels.formula.api as sm\n",
        "\"\"\"Since the eqaution is y=bo + b1x1+------+bnxn there is no \n",
        "x0 in the datset so we will add one\"\"\"\n",
        "x=np.append(arr =np.ones((50,1)).astype(int),values=x,axis=1)\n",
        "#we just add a column of ones in fornt of the existing x\n",
        "\n",
        "x_opt = x[:,[0,3]]\n",
        "regressor_OLS=sm.OLS(endog =y,exog=x_opt).fit()\n",
        "\n",
        "print(regressor_OLS.summary())\n",
        "#watch the video number 045-046 again as we will remove the different columns\n",
        "#on the bassi of p value of different variable shown .summary() method"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      y   R-squared:                       0.947\n",
            "Model:                            OLS   Adj. R-squared:                  0.945\n",
            "Method:                 Least Squares   F-statistic:                     849.8\n",
            "Date:                Sun, 17 Mar 2019   Prob (F-statistic):           3.50e-32\n",
            "Time:                        11:37:41   Log-Likelihood:                -527.44\n",
            "No. Observations:                  50   AIC:                             1059.\n",
            "Df Residuals:                      48   BIC:                             1063.\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const       4.903e+04   2537.897     19.320      0.000    4.39e+04    5.41e+04\n",
            "x1             0.8543      0.029     29.151      0.000       0.795       0.913\n",
            "==============================================================================\n",
            "Omnibus:                       13.727   Durbin-Watson:                   1.116\n",
            "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               18.536\n",
            "Skew:                          -0.911   Prob(JB):                     9.44e-05\n",
            "Kurtosis:                       5.361   Cond. No.                     1.65e+05\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The condition number is large, 1.65e+05. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}